{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-tenant Chat with Papers - Query papers\n",
    "## Get keys and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate Key:root-user-key\n",
      "OpenAI API Key: sk-proj-iuwKF1Q94jnW\n",
      "OpenAI URL: https://api.openai.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_URL = os.getenv(\"OPENAI_URL\")\n",
    "\n",
    "print(f\"Weaviate Key:{WEAVIATE_KEY}\")\n",
    "print(f\"OpenAI API Key: {OPENAI_API_KEY[:20]}\")\n",
    "print(f\"OpenAI URL: {OPENAI_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Connect to the local instance\n",
    "client = weaviate.connect_to_local(\n",
    "  host=\"127.0.0.1\", # the address to the learner's instance\n",
    "  port=8080,\n",
    "  grpc_port=50051,\n",
    "  auth_credentials=Auth.api_key(WEAVIATE_KEY),\n",
    "  headers={\n",
    "    \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "  }\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search on tenants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,\n",
      "and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token\n",
      "prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the pro- cessing of learning to follow instructions. Generative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identiﬁers, such as id and sub-string, which map directly to realdocuments. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua\n",
      "standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten.query.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Search with tenants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "The text describes methods and advancements in self-supervised representation learning, particularly in the context of natural language processing (NLP) using generative large language models (LLMs). It highlights two distinct learning approaches enabled by modern deep learning: \n",
      "\n",
      "1. **Token-Level Learning:** This refers to generative LLMs that are pre-trained on large text corpora, showcasing their capabilities in natural language understanding (NLU) and natural language generation (NLG). These models can process and generate human-like text based on the input data they have seen during training.\n",
      "\n",
      "2. **Document-Level Learning:** This aspect involves text encoders that use contrastive learning objectives to capture document-document similarities. This enables the model to understand how similar or different documents are by encoding this information into a mathematical representation (inner-product).\n",
      "\n",
      "Additionally, the text notes an insight about LLMs being further trained to follow instructions, which allows them to perform well on unseen tasks without needing specific examples—a concept known as zero-shot generalization.\n",
      "\n",
      "Overall, the passage illustrates the state-of-the-art techniques in the field of NLP that leverage self-supervised learning and emphasizes the transition from supervised to self-supervised paradigms in training AI models. \n",
      "\n",
      "many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,\n",
      "The text describes research focused on developing effective dense retrieval systems in information retrieval, specifically under scenarios where relevance labels (which indicate the usefulness or appropriateness of retrieved documents) are not available. It contrasts this approach with traditional methods that typically rely on large, well-supervised datasets like MS-MARCO for training dense retrievers.\n",
      "\n",
      "The authors recognize that, as referenced by Izacard et al. (2021), the assumption of having access to a large, richly supervised corpus is not always realistic. Therefore, this research aims to investigate methods for building retrieval systems that are more practical and robust by not relying on such extensive labeled data, especially during the training phase. By working without the assumption of access to test corpora for training, the authors intend to avoid the pitfalls of over-engineering solutions based on specific test conditions, making their methodology align more closely with real-world applications where labeled data may be scarce. The study thus seeks to contribute to the understanding of unsupervised or less-supervised learning methods in dense retrieval contexts. \n",
      "\n",
      "and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token\n",
      "The text describes research focused on developing zero-shot dense retrieval systems that operate without the need for relevance supervision or human-annotated datasets. It references the MS-MARCO dataset, which is a well-known collection of query-document pairs used for training and evaluating information retrieval systems. The paper aims to explore self-supervised representation learning techniques to enable these retrieval systems to generalize across different tasks and work effectively without requiring task-specific training data. This research highlights the challenges of using large annotated datasets in practical applications and seeks to address those challenges through innovative machine learning approaches. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten2212 = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten2212.generate.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=3,\n",
    "    single_prompt=\"What does the following text describe: {chunk}\",\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])\n",
    "    print(item.generative.text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,\n",
      "and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token\n",
      "Unsupervised learning is a machine learning approach where the model learns patterns and representations from data without labeled supervision. In this context, self-supervised representation learning methods are leveraged. \n",
      "\n",
      "Modern deep learning encompasses two main learning algorithms relevant to unsupervised learning: \n",
      "\n",
      "1. **Token Level**: Generative large language models (LLMs) pretrained on extensive corpuses exhibit robust capabilities in natural language understanding (NLU) and generation (NLG). These models learn to predict or generate text based on the patterns observed in the input data without explicit labels.\n",
      "\n",
      "2. **Document Level**: Text (chunk) encoders trained with contrastive objectives learn to encode the similarity between documents based on their representations. This approach contrasts pairs of documents to learn what makes them similar or different, aiding in the evaluation of document-document similarity.\n",
      "\n",
      "The absence of supervision leads researchers to focus on techniques that build effective retrieval systems without relevance labels or access to test-time corpora. This setup is regarded as \"unsupervised\" as it operates under the premise that relevance judgments may not always be available. \n",
      "\n",
      "Overall, in an unsupervised learning framework, models adapt and generalize across diverse tasks by relying on intrinsic patterns and structures within the data rather than on explicitly labeled examples.\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten2212 = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten2212.generate.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=3,\n",
    "    grouped_task=\"Explain how unsupervised learning works. Use only the provided content.\",\n",
    "    grouped_properties=[\"chunk\"]\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])\n",
    "\n",
    "print(response.generative.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_rag(paper_id, query, prompt):\n",
    "    papers = client.collections.get(\"Papers\")\n",
    "    ten = papers.with_tenant(paper_id)\n",
    "\n",
    "    response = ten.generate.near_text(\n",
    "        query=query,\n",
    "        limit=3,\n",
    "        grouped_task=prompt + \" Use only the provided content.\",\n",
    "        grouped_properties=[\"chunk\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"title\": response.objects[0].properties[\"title\"],\n",
    "        \"source\": [p.properties[\"chunk\"] for p in response.objects],\n",
    "        \"generated\": response.generative.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Precise Zero-Shot Dense Retrieval without Relevance Labels',\n",
       " 'source': ['supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang',\n",
       "  'many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,',\n",
       "  'and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token'],\n",
       " 'generated': 'Unsupervised learning is a machine learning approach where models are trained without labeled data or explicit supervision. Instead, it relies on discovering patterns, structures, or insights from the data itself. In the context of representation learning, techniques such as self-supervised representation learning are utilized. \\n\\nIn unsupervised learning for natural language processing, modern deep learning algorithms are employed. For instance, large language models (LLMs) trained on vast corpora exhibit strong capabilities in natural language understanding (NLU) and generation (NLG). These models can be further fine-tuned to follow specific instructions, allowing them to generalize to unseen tasks effectively.\\n\\nUnsupervised learning is particularly useful when labeled data is scarce or unavailable, as is often the case in real-world applications. For example, some studies focus on building dense retrieval systems that do not rely on relevance labels for training. This approach allows for more realistic setups and avoids the complications of relying solely on extensive supervised datasets, like the MS-MARCO collection, which may not always be accessible or applicable to specific scenarios.\\n\\nIn summary, unsupervised learning operates by leveraging inherent data structures and patterns, making it a valuable method when traditional supervision is not feasible.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_rag(\n",
    "    \"2212-10496\",\n",
    "    \"Unsupervised learning\",\n",
    "    \"Explain how unsupervised learning works\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2212-10496': TenantOutput(name='2212-10496', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>),\n",
       " '2401-00107': TenantOutput(name='2401-00107', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "papers.tenants.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
